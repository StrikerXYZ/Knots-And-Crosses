{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT796_Task_2.2C.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OFxKS7KnwEG"
      },
      "source": [
        "# **Task 2.2C**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abop9d3rrbjc"
      },
      "source": [
        "GitHub: https://github.com/StrikerXYZ/Knots-And-Crosses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAiOX9DWZHfL"
      },
      "source": [
        "# install required system dependencies\n",
        "!apt-get install -y xvfb x11-utils  \n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            gym[box2d]==0.17.* \\\n",
        "!pip install pyglet\n",
        "!pip install ffmpeg\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install Image\n",
        "!pip install gym-maze-trustycoder83"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYLhSf9jP15B"
      },
      "source": [
        "!mkdir ./vid\n",
        "!rm ./vid/*.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhkSJE1LkdsZ"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import gym\n",
        "import gym_maze\n",
        "import base64\n",
        "import io\n",
        "\n",
        "from IPython import display\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "\n",
        "d = Display()\n",
        "d.start()\n",
        "\n",
        "env = gym.make(\"maze-sample-10x10-v0\")\n",
        "video_name = \"./vid/Practical_2.mp4\"\n",
        "vid = None\n",
        "\n",
        "def startVideo():\n",
        "  return video_recorder.VideoRecorder(env,video_name)\n",
        "\n",
        "def endVideo(vid):\n",
        "  vid.close()\n",
        "  vid.enabled = False\n",
        "  video = io.open(video_name, 'r+b').read()\n",
        "  encoded = base64.b64encode(video)\n",
        "  display.display(display.HTML(data=\"\"\"\n",
        "    <video alt=\"test\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "    </video>\n",
        "    \"\"\".format(encoded.decode('ascii'))))\n",
        "\n",
        "def test(vid, n_episodes, max_iter_episode, epsilon, gamma, optimisticInit):\n",
        "  current_state = env.reset()\n",
        "\n",
        "  states_dic = {} #dictionary to keep the states/coordinates of the Q table\n",
        "  count = 0\n",
        "  for i in range(10):\n",
        "      for j in range(10):\n",
        "          states_dic[i, j] = count\n",
        "          count+=1\n",
        "          \n",
        "  n_actions = env.action_space.n\n",
        "\n",
        "  # Initialize the Q-table to 0\n",
        "  Q_table = np.zeros((len(states_dic), n_actions))\n",
        "\n",
        "  if optimisticInit:\n",
        "    Q_table = np.ones((len(states_dic), n_actions))\n",
        "\n",
        "  # Number of episode we will run\n",
        "  #n_episodes = 10\n",
        "\n",
        "  # Maximum of iteration per episode\n",
        "  #max_iter_episode = 100\n",
        "\n",
        "  # Epsilon\n",
        "  #epsilon = 0.5\n",
        "  #epsilon_decay = 0.001\n",
        "  #min_epsilon = 0.01\n",
        "\n",
        "  # Learning rate\n",
        "  gamma = 0.7\n",
        "\n",
        "  rewards_per_episode = list()\n",
        "\n",
        "  # Iterate over episodes\n",
        "  for e in range(n_episodes):\n",
        "      \n",
        "      # We are not done yet\n",
        "      done = False\n",
        "      \n",
        "      # Sum the rewards that the agent gets from the environment\n",
        "      total_episode_reward = 0\n",
        "\n",
        "      n_a = 0;\n",
        "\n",
        "      #last_reward = 0\n",
        "\n",
        "      for i in range(max_iter_episode): \n",
        "          env.unwrapped.render()\n",
        "          vid.capture_frame()\n",
        "          current_coordinate_x = int(current_state[0])\n",
        "          current_coordinate_y = int(current_state[1])\n",
        "          current_Q_table_coordinates = states_dic[current_coordinate_x, current_coordinate_y]\n",
        "\n",
        "          explore = np.random.uniform(0,1) < epsilon\n",
        "          if explore:\n",
        "            action = env.action_space.sample()\n",
        "          else:\n",
        "            action = int(np.argmax(Q_table[current_Q_table_coordinates]))\n",
        "            \n",
        "          #print(\"Explore: \", explore)\n",
        "          #print(current_Q_table_coordinates, \">>>\", Q_table[current_Q_table_coordinates], \" ==> \", action)\n",
        "\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "          next_coordinate_x = int(next_state[0]) #get coordinates to be used in dictionary\n",
        "          next_coordinate_y = int(next_state[1]) #get coordinates to be used in dictionary\n",
        "\n",
        "\n",
        "          # Update our Q-table using the Q-learning iteration\n",
        "          next_Q_table_coordinates = states_dic[next_coordinate_x, next_coordinate_y]\n",
        "          #Q_table[current_Q_table_coordinates, action] = (1-lr) *Q_table[current_Q_table_coordinates, action] +lr*(reward + max(Q_table[next_Q_table_coordinates,:]))\n",
        "\n",
        "          #increment N(A)\n",
        "          n_a += 1;\n",
        "          alpha = 1/n_a;\n",
        "\n",
        "          #Evaluate Q value\n",
        "          q = Q_table[current_Q_table_coordinates, action];\n",
        "          q_max = np.amax(Q_table[next_Q_table_coordinates]);\n",
        "          q += alpha * (reward - gamma*q)\n",
        "          #q += alpha * (reward + gamma*q_max - q)\n",
        "          Q_table[current_Q_table_coordinates, action] = q;\n",
        "          #print(\"after: \", Q_table[current_Q_table_coordinates])\n",
        "\n",
        "          #Decay epsilon\n",
        "          #epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay))\n",
        "          #last_reward = q\n",
        "          total_episode_reward = total_episode_reward + reward\n",
        "          # If the episode is finished, we leave the for loop\n",
        "          if done:\n",
        "              break\n",
        "          current_state = next_state\n",
        "\n",
        "      #Show the total episode reward        \n",
        "      #print(\"Total episode reward:\", total_episode_reward)\n",
        "      \n",
        "      #Reset enviroment for next episode\n",
        "      current_state = env.reset()\n",
        "      \n",
        "      rewards_per_episode.append(total_episode_reward)\n",
        "  \n",
        "  return rewards_per_episode\n",
        "\n",
        "#vid = startVideo()\n",
        "#test(vid, 10, 1000, 0.5, 1, True)\n",
        "#endVideo(vid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9bbG5htaC8"
      },
      "source": [
        "We can now play the video using the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0oEe_YSnKuY"
      },
      "source": [
        "import matplotlib.pyplot as mp\n",
        "\n",
        "vid = startVideo()\n",
        "\n",
        "# Number of episode we will run\n",
        "n_episodes = 10\n",
        "# Maximum of iteration per episode\n",
        "max_iter_episode = 1000\n",
        "\n",
        "epsilonList = list()\n",
        "start = 0\n",
        "inc = 0.2\n",
        "for i in range(6):\n",
        "  epsilonList.append(start + i * inc);\n",
        "\n",
        "print(\"Epsilon Report:\")\n",
        "labelTxt = \"eps = {v:.1f}\"\n",
        "for i in range(6):\n",
        "  epsilonResult = test(vid, n_episodes, max_iter_episode, epsilonList[i], 0.5, False)\n",
        "  mp.plot( np.arange(0, n_episodes), epsilonResult, 'o--', linewidth=1, markersize=10, label=labelTxt.format(v = epsilonList[i]))\n",
        "mp.xlabel('episodes')\n",
        "mp.ylabel('reward')\n",
        "mp.legend(loc='best')\n",
        "mp.show()\n",
        "\n",
        "\n",
        "gammaList = list()\n",
        "start = 0\n",
        "inc = 0.2\n",
        "for i in range(6):\n",
        "  gammaList.append(start + i * inc);\n",
        "\n",
        "print(\"Gamma Report:\")\n",
        "labelTxt = \"gamma = {v:.1f}\"\n",
        "for i in range(6):\n",
        "  gammaResult = test(vid, n_episodes, max_iter_episode, 0.5, gammaList[i], False)\n",
        "  mp.plot( np.arange(0, n_episodes), gammaResult, 'o--', linewidth=1, markersize=10, label=labelTxt.format(v = gammaList[i]))\n",
        "mp.xlabel('episodes')\n",
        "mp.ylabel('reward')\n",
        "mp.legend(loc='best')\n",
        "mp.show()\n",
        "\n",
        "print(\"Optimistic vs Realistic:\")\n",
        "\n",
        "realisticResult = test(vid, n_episodes, max_iter_episode, 0.5, 0.5, False)\n",
        "mp.plot( np.arange(0, n_episodes), realisticResult, 'o--', linewidth=1, markersize=10, label=\"realistic\")\n",
        "optimisticResult = test(vid, n_episodes, max_iter_episode, 0.5, 0.5, False)\n",
        "mp.plot( np.arange(0, n_episodes), optimisticResult, 'o--', linewidth=1, markersize=10, label=\"optimistic\")\n",
        "mp.xlabel('episodes')\n",
        "mp.ylabel('reward')\n",
        "mp.legend(loc='best')\n",
        "mp.show()\n",
        "\n",
        "endVideo(vid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxxPLPmX-SkM"
      },
      "source": [
        "Discussion:\n",
        "\n",
        "- In the first evaluation the lower epsilon seemed to provided a better results at the earliest of the iterations while the higher epsilon provided better results towards more iterations. Lower epsilon resulted in less exploring but seems more likely to find a lower minimum quickly which in this maze may be global minimum. This is most likely why the lower epsilon may find better results at lower iterations. Higher epsilon may explore the grid a lot more and would seem to take more educated actions in the higher iterations\n",
        "\n",
        "- In the second evaulation the the lower gamma/ learning rate seems to provide a smoother curve compared to the higher gamma results. The lower gamma seems to be able to converge on the results quicker while the higher gamma seems to have larger jumps before it can settle towards better rewards\n",
        "\n",
        "- The last evaluation of realistic vs optimisic shows that realistic method may find the rewards more consistently while optimistic approach is able to find higher rewards very early on."
      ]
    }
  ]
}